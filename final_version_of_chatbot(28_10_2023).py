# -*- coding: utf-8 -*-
"""Final_version_of_Chatbot(28/10/2023).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yjEZ24_vgPR188S3rLBr2b5UPGaOXSrd

# **Import libraries (CLEAR)**
"""

import nltk
nltk.download('punkt')
from nltk.stem.porter import PorterStemmer
import numpy as np
import json
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout, Dense, Input
from tensorflow.keras.utils import to_categorical
import random

"""# **Dataset example (CLEAR)**"""

dataset = {
  "intents": [
    {
      "tag": "greeting",
      "patterns": [
        "Hi",
        "Hey",
        "How are you",
        "Is anyone there?",
        "Hello",
        "Good day"
      ],
      "responses": [
        "Hey :-)",
        "Hello, thanks for visiting",
        "Hi there, what can I do for you?",
        "Hi there, how can I help?"
      ]
    },
    {
      "tag": "goodbye",
      "patterns": ["Bye", "See you later", "Goodbye"],
      "responses": [
        "See you later, thanks for visiting",
        "Have a nice day",
        "Bye! Come back again soon."
      ]
    },
    {
      "tag": "thanks",
      "patterns": ["Thanks", "Thank you", "That's helpful", "Thank's a lot!"],
      "responses": ["Happy to help!", "Any time!", "My pleasure"]
    },
    {
      "tag": "items",
      "patterns": [
        "Which items do you have?",
        "What kinds of items are there?",
        "What do you sell?"
      ],
      "responses": [
        "We sell coffee and tea",
        "We have coffee and tea"
      ]
    },
    {
      "tag": "payments",
      "patterns": [
        "Do you take credit cards?",
        "Do you accept Mastercard?",
        "Can I pay with Paypal?",
        "Are you cash only?"
      ],
      "responses": [
        "We accept VISA, Mastercard and Paypal",
        "We accept most major credit cards, and Paypal"
      ]
    },
    {
      "tag": "delivery",
      "patterns": [
        "How long does delivery take?",
        "How long does shipping take?",
        "When do I get my delivery?"
      ],
      "responses": [
        "Delivery takes 2-4 days",
        "Shipping takes 2-4 days"
      ]
    },
    {
      "tag": "funny",
      "patterns": [
        "Tell me a joke!",
        "Tell me something funny!",
        "Do you know a joke?"
      ],
      "responses": [
        "Why did the hipster burn his mouth? He drank the coffee before it was cool.",
        "What did the buffalo say when his son left for college? Bison."
      ]
    }
  ]
}

file_path = "data.json"
with open(file_path, 'w') as file:
    json.dump(dataset, file)
file.close()

"""# **Pre-processing (CLEAR)**"""

def tokenize(sentence):
  return nltk.word_tokenize(sentence)

# Example
sentence = "You're learning DL or ML?"
tokenize(sentence)

def lower_and_stem(word, stemmer = None):
  stemmer = PorterStemmer()
  return stemmer.stem(word.lower())

# Example
words = ['organize', 'organizes', 'organizing']
res = [lower_and_stem(word) for word in words]
res

def bag_of_words(sentence, words):
  """
  Input:
      sentence = ['why', 'hello', 'there']
      words = ['why', 'how', 'what', 'hello', 'good', 'there', 'huh']
  Output:
      bag =   [  1,     0,     0,       1,      0,       1,      0]
  """
  bag = np.zeros(len(words), dtype = np.float32)
  for i in range(len(sentence)):
    for j in range(len(words)):
      if sentence[i] == words[j]:
        bag[j] = 1
  return bag

# Example
sentence = ['why', 'hello', 'there']
words = ['why', 'how', 'what', 'hello', 'good', 'there', 'huh']
bag_of_words(sentence, words)

with open('data.json', 'r') as data:
  intents = json.load(data)
intents

def preprocessing(intents):
  all_words = []
  tags = []
  xy = []
  punctuations = [',', '!', '?', '.']
  for intent in intents['intents']:
    tag = intent['tag']
    if tag not in tags:
      tags.append(tag)

    for pattern in intent['patterns']:
      words = tokenize(pattern)
      words = [word for word in words if word not in punctuations]
      all_words.extend(words)
      xy.append((words, tag))

  all_words = sorted(set(all_words))
  tags = sorted(set(tags))

  tag_encode = {}
  for i in range(len(tags)):
    tag_encode[tags[i]] = i

  tag_decode = {value: key for key, value in tag_encode.items()}


  return all_words, xy, tag_encode, tag_decode, tags

all_words, xy, tag_encode, tag_decode, tags = preprocessing(intents)

def training_data_for_chatbot(xy, all_words, tag_encode):
  x_train = []
  for (pattern_sentence, _) in xy:
    x_train.append(bag_of_words(pattern_sentence, all_words))
  x_train = np.array(x_train)

  y_train = []
  for (_, tag) in xy:
    y_train.append(tag_encode[tag])
  y_train = to_categorical(y_train)

  return x_train, y_train

x_train, y_train = training_data_for_chatbot(xy, all_words, tag_encode)

"""# **Feed Forward Neural Network (CLEAR)**"""

model = Sequential()
model.add(Input(shape = (len(x_train[1]), )))
model.add(Dense(32, activation = 'relu'))
model.add(Dense(64, activation = 'relu'))
model.add(Dense(128, activation = 'relu'))
model.add(Dense(len(y_train[1]), activation = 'softmax'))

model.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')

history = model.fit(x_train, y_train, epochs = 200, batch_size = 32)

import matplotlib.pyplot as plt

# Extract loss and accuracy from the history object
loss = history.history['loss']
accuracy = history.history['accuracy']

# Create subplots for loss and accuracy
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))

# Plot loss
ax1.plot(loss, label='Training Loss')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss')
ax1.legend()

# Plot accuracy
ax2.plot(accuracy, label='Training Accuracy')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Accuracy')
ax2.legend()

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

"""# **Predict (CLEAR)**"""

def predict(text):
  punctuations = [',', '!', '?', '.']
  words = tokenize(text)
  words = [lower_and_stem(word) for word in words if word not in punctuations]
  bag = bag_of_words(words, all_words).reshape(1, -1)
  return np.argmax(model.predict(bag))

def response(text, tag_decode, intents):
  output = predict(text)
  tag = tag_decode[output]
  for i in range(len(intents['intents'])):
    if intents['intents'][i]['tag'] == tag:
      response = random.choice(intents['intents'][i]['responses'])
  return response

"""# **Deploy (CLEAR)**"""

print("GO! Bot is running")
while True:
  message = input("")
  if message == "EOC":
    break
  else:
    print(response(message, tag_decode, intents))